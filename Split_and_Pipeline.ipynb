{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-omCTziNIFC3"
      },
      "source": [
        "#Pipeline + Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1BlJNkb8xL4"
      },
      "source": [
        "<a id=\"0\"></a> <br>\n",
        " ## Notebook Plan  \n",
        "1. [Libraries import](#libraries_import)     \n",
        "1. [Function Declaration](#functions)\n",
        "1. [Data Loading and Transformation](#data)\n",
        "1. [Feature Engineering](#feature)     \n",
        "1. [Ml Pipeline](#pipeline)   \n",
        "1. [Statistics on days](#stat)   \n",
        "1. [Splitting](#split)   \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVFTBv5uIF7Q"
      },
      "source": [
        "\n",
        "## Importing necesssary libraries <a class=\"anchor\" id=\"libraries_import\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsOWMzLScPw5"
      },
      "outputs": [],
      "source": [
        "# !pip install pyspark py4j pyarrow\n",
        "# !pip install implicit\n",
        "# !pip install sparktorch\n",
        "\n",
        "\n",
        "import random\n",
        "import os\n",
        "import sys\n",
        "import math as m\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import plotly\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "import plotly.figure_factory as ff\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#pyspark\n",
        "from pyspark.streaming import StreamingContext\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import monotonically_increasing_id, pandas_udf,regexp_replace,col\n",
        "from pyspark.sql.types import IntegerType, FloatType, BooleanType, DateType, StringType\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "#pyspark ml\n",
        "from pyspark.ml.functions import vector_to_array\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import OneHotEncoder,StringIndexer,QuantileDiscretizer, OneHotEncoder, StandardScaler, StringIndexer, VectorAssembler\n",
        "\n",
        "#imlicit\n",
        "from implicit.als import AlternatingLeastSquares\n",
        "from implicit.nearest_neighbours import tfidf_weight\n",
        "\n",
        "#torch\n",
        "from sparktorch import serialize_torch_obj, SparkTorch, PysparkPipelineWrapper\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTiN8vAWqNpG"
      },
      "source": [
        "## Declaring functions <a class=\"anchor\" id=\"functions\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBXZqWIePgwr"
      },
      "outputs": [],
      "source": [
        "def OneHotEncoding(dataframe: pd.DataFrame, input_col: pd.Series) -> pd.DataFrame:\n",
        "\n",
        "  indexer = StringIndexer(inputCol=input_col, outputCol='output_1')\n",
        "  indexer_fitted = indexer.fit(dataframe)\n",
        "  df_indexed = indexer_fitted.transform(dataframe)\n",
        "\n",
        "  encoder = OneHotEncoder(inputCols=['output_1'], outputCols=['output_2'], dropLast=False)\n",
        "  df_onehot = encoder.fit(df_indexed).transform(df_indexed)\n",
        "\n",
        "  df_col_onehot = df_onehot.select('*', vector_to_array('output_2').alias('output_3'))\n",
        "\n",
        "  num_categories = len(df_col_onehot.first()['output_3'])\n",
        "  cols_expanded = [(F.col('output_3')[i].alias(f'{indexer_fitted.labels[i]}')) for i in range(num_categories)]\n",
        "  df_cols_onehot = df_col_onehot.select('*', *cols_expanded)\n",
        "  df_cols_onehot = df_cols_onehot.select([column for column in df_cols_onehot.columns if column not in ['output_1','output_2','output_3']])\n",
        "  return df_cols_onehot\n",
        "\n",
        "def delete_brackets(dataframe: pd.DataFrame, column: pd.Series) -> pd.Series:\n",
        "    df = dataframe.withColumn(column, F.translate(column, '[]', ' '))\n",
        "    return df\n",
        "\n",
        "def change_datatype_of_ohe(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
        "  for col,col_type in dataframe.dtypes:\n",
        "    if col_type == 'double':\n",
        "      dataframe = dataframe.withColumn(col, F.col(col).cast(BooleanType()))\n",
        "  return dataframe\n",
        "\n",
        "def avg_session_duration(dataframe:pd.DataFrame):\n",
        "  df3 = dataframe.groupBy(\"user_id\",'session_duration').agg(F.countDistinct('session_duration'))\n",
        "  w = Window.partitionBy(\"user_id\").orderBy('user_id')\n",
        "  df3 = df3.withColumn('avg_session_duration', F.from_unixtime(F.sum(F.to_timestamp(F.col('session_duration')).cast('long')).over(w) / F.sum(F.to_timestamp(F.col('count(session_duration)')).cast('long')).over(w),\"HH:mm:ss\"))\n",
        "  df3 = df3.select('user_id','avg_session_duration')\n",
        "  dataframe = dataframe.join(df3, on=[\"user_id\"],how='left')\n",
        "  df3.unpersist()\n",
        "\n",
        "  return dataframe\n",
        "\n",
        "def day_time(dataframe:pd.DataFrame):\n",
        "  dataframe = dataframe.withColumn('day_time', F.when(F.hour(F.to_timestamp(F.col('utc_event_time'))).between(6, 8),'early morning') \\\n",
        "                            .when(F.hour(F.to_timestamp(F.col('utc_event_time'))).between(9, 11),'morning') \\\n",
        "                            .when(F.hour(F.to_timestamp(F.col('utc_event_time'))).between(12, 16),'afternoon') \\\n",
        "                            .when(F.hour(F.to_timestamp(F.col('utc_event_time'))).between(17, 23),'evening') \\\n",
        "                            .otherwise('night'))\n",
        "  return dataframe\n",
        "\n",
        "def day_of_week(dataframe:pd.DataFrame):\n",
        "  dataframe = dataframe.withColumn('day_of_week', F.dayofweek(F.col('utc_event_date')))\n",
        "\n",
        "  return dataframe\n",
        "\n",
        "\n",
        "def mean_product_price_per_person(dataframe:pd.DataFrame):\n",
        "  df3 = dataframe.groupBy(\"user_id\").agg(F.mean('price').alias(\"mean_price\"))\n",
        "  dataframe = dataframe.join(df3, on=[\"user_id\"],how='left')\n",
        "  df3.unpersist()\n",
        "\n",
        "  return dataframe\n",
        "\n",
        "\n",
        "def interacted_categories_per_person(dataframe:pd.DataFrame):\n",
        "  df3 = dataframe.groupBy(\"user_id\").agg(F.countDistinct('main_category').alias(\"inter_categ\"))\n",
        "  dataframe = dataframe.join(df3, on=[\"user_id\"],how='left')\n",
        "  df3.unpersist()\n",
        "\n",
        "  return dataframe\n",
        "\n",
        "def interacted_goods_per_person(dataframe:pd.DataFrame):\n",
        "  df3 = dataframe.groupBy(\"user_id\").agg(F.countDistinct('product_id').alias(\"inter_goods\"))\n",
        "  dataframe = dataframe.join(df3, on=[\"user_id\"],how='left')\n",
        "  df3.unpersist()\n",
        "\n",
        "  return dataframe\n",
        "\n",
        "def avg_goods_per_person(dataframe:pd.DataFrame):\n",
        "  df3 = dataframe.groupBy(\"user_id\",'session_id').agg(F.countDistinct('product_id').alias(\"num_of_goods\"))\n",
        "  expr = [F.count(F.col(\"num_of_goods\")),F.count(F.col(\"session_id\"))]\n",
        "  df3 = df3.groupBy(\"user_id\").agg(*expr)\n",
        "  df3 = df3.withColumn('avg_goods_per_session', F.col('count(num_of_goods)') / F.col('count(session_id)'))\n",
        "  dataframe = dataframe.join(df3.select([\"user_id\",'avg_goods_per_session']), on=[\"user_id\"],how='left')\n",
        "  df3.unpersist()\n",
        "\n",
        "  return dataframe\n",
        "\n",
        "def change_dtype(dataframe:pd.DataFrame, int_cols:list, string_cols:list, date_cols:list):\n",
        "  for column in dataframe.columns:\n",
        "    if column in int_cols:\n",
        "      dataframe = dataframe.withColumn(col,F.col(column).cast(IntegerType()))\n",
        "    elif column in string_cols:\n",
        "      dataframe = dataframe.withColumn(col,F.to_timestamp(column).cast(StringType()))\n",
        "    elif column in date_cols:\n",
        "      dataframe = dataframe.withColumn(col,F.to_timestamp(column).cast(DateType()))\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  return dataframe\n",
        "\n",
        "def split_on_sessions(dataframe:pd.DataFrame):\n",
        "  #time dofference with previous event\n",
        "  window = Window.partitionBy(\"user_id\").orderBy(\"user_id\",\"utc_event_time\")\n",
        "  df_new = dataframe.withColumn(\"id\", monotonically_increasing_id()) # add id\n",
        "  df.unpersist()\n",
        "  df_new = df_new.withColumn(\"prev_value\", F.lag(F.to_timestamp(F.col(\"utc_event_time\"))).over(window)) #keep previous records\n",
        "  df_new = df_new.withColumn(\"min_diff\", F.when(F.isnull(F.to_timestamp(F.col(\"utc_event_time\")).cast(\"long\") - F.col(\"prev_value\").cast(\"long\")), np.nan) #count difference in minutes\n",
        "                                .otherwise(F.from_unixtime((F.to_timestamp(F.col(\"utc_event_time\")).cast(\"long\") - F.to_timestamp(F.col(\"prev_value\")).cast(\"long\")), \"HH:mm:ss\")))\n",
        "\n",
        "\n",
        "  # df with starts of sessions\n",
        "  sessions_start_df = df_new[(F.col('min_diff') == np.nan) | (F.col('min_diff') > '00:30:00' )]\n",
        "  sessions_start_df = sessions_start_df.withColumn(\"session_id\", F.col('id'))\n",
        "  sessions_start_df = sessions_start_df.select('id','session_id')\n",
        "\n",
        "\n",
        "  #numbering sessions\n",
        "  df_new = df_new.join(sessions_start_df, on=[\"id\"],how='left')\n",
        "  sessions_start_df.unpersist()\n",
        "\n",
        "  w = Window.orderBy('user_id','utc_event_time')\n",
        "  df_new = df_new.withColumn('session_id', F.when(F.isnull(F.col('session_id')),F.last('session_id', True).over(w)).otherwise(F.col('session_id')))\n",
        "  df_new = df_new.sort(F.col('user_id'),F.col('utc_event_time'))\n",
        "\n",
        "\n",
        "  # start of the session\n",
        "  df_new = df_new.withColumn(\"is_first_event_in_session\", F.col('id') == F.col('session_id'))\n",
        "\n",
        "\n",
        "  # session duration\n",
        "  expr = [F.min(F.col(\"utc_event_time\")),F.max(F.col(\"utc_event_time\"))]\n",
        "  df2 = df_new.groupBy(\"session_id\").agg(*expr)\n",
        "  df2 = df2.withColumn('session_duration',F.from_unixtime(F.to_timestamp(F.col('max(utc_event_time)')).cast('long') - F.to_timestamp(F.col('min(utc_event_time)')).cast('long'),\"HH:mm:ss\"))\n",
        "  df2 = df2.select('session_id','session_duration')\n",
        "  df_new = df_new.join(df2, on=[\"session_id\"],how='left').drop('id','min_diff','prev_value')\n",
        "  df2.unpersist()\n",
        "\n",
        "  return df_new\n",
        "\n",
        "def product_popularity(dataframe: pd.DataFrame):\n",
        "  num_of_users = dataframe.select('user_id').distinct().count()\n",
        "  df1 = dataframe.groupBy('product_id').agg(F.count_distinct(F.col('user_id')) / num_of_users)\n",
        "  dataframe = dataframe.join(df1, on=['product_id'], how = 'left')\n",
        "  df1.unpersist()\n",
        "\n",
        "  return dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkHkanwwqRpe"
      },
      "source": [
        "## Loading & transforming DataFrame <a class=\"anchor\" id=\"data\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvcwpmQwH54G"
      },
      "source": [
        "Mounting a csv-file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX6DRlzBcFYU",
        "outputId": "93cfb6c8-bdfa-4952-8fb5-ced6a48cb66e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Au10TL160akL",
        "outputId": "5835265d-12d2-4e2f-fb24-25dd92ee0714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Colab Notebooks/data_v2_new.zip\n",
            "  inflating: ab_data_new.csv         \n",
            "Archive:  /content/drive/MyDrive/Colab Notebooks/data_v2_old.zip\n",
            "  inflating: ab_data_old.csv         \n"
          ]
        }
      ],
      "source": [
        "!unzip '/content/drive/MyDrive/Colab Notebooks/data_v2_new.zip'\n",
        "!unzip '/content/drive/MyDrive/Colab Notebooks/data_v2_old.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG5tjUVoI1cf"
      },
      "source": [
        "Creating a PySpark session and getting a DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "qMzt8USYJgQ8",
        "outputId": "0514a79e-6084-4035-efe8-8968b4abd862"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x78b57359f9a0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://10e8f8d73c99:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Model</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
        "\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .appName('Model')\\\n",
        "        .master('local[*]') \\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark.sparkContext.setCheckpointDir(\"/content/drive/MyDrive/Colab Notebooks/spark_checkpoints\")\n",
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(\n",
        "    [(\"a\", 1, 0), (\"a\", -1, 42), (\"b\", 3, -1), (\"b\", 10, -2),(\"c\", 10, -2),(\"d\", 10, -2),(\"e\", 10, -2),(\"f\", 10, -2),(\"g\", 10, -2)],\n",
        "    (\"key\", \"value1\", \"value2\")\n",
        ")"
      ],
      "metadata": {
        "id": "y7gq88HH3Jbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFg6giXQ3Jk5",
        "outputId": "8077c362-2f78-4e59-e071-7f13a008911b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+------+\n",
            "|key|value1|value2|\n",
            "+---+------+------+\n",
            "|  a|     1|     0|\n",
            "|  a|    -1|    42|\n",
            "|  b|     3|    -1|\n",
            "|  b|    10|    -2|\n",
            "|  c|    10|    -2|\n",
            "|  d|    10|    -2|\n",
            "|  e|    10|    -2|\n",
            "|  f|    10|    -2|\n",
            "|  g|    10|    -2|\n",
            "+---+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rowwise_function(df):\n",
        "  a = df.select(F.min(F.col('value1'))).collect()[0][0]\n",
        "  # + random.randint(1,10)\n",
        "  return a"
      ],
      "metadata": {
        "id": "PnYK__V63JwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df\n",
        "# apply our function to RDD\n",
        "cases_rdd_new = df.rdd.map(lambda row: rowwise_function(df1))\n",
        "# Convert RDD Back to DataFrame\n",
        "casesNewDf = spark.createDataFrame(cases_rdd_new)\n",
        "casesNewDf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "H6j4XXsc3JrJ",
        "outputId": "7baeadd2-1b4c-4774-87a5-c8dbb8f392c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/serializers.py\", line 459, in dumps\n",
            "    return cloudpickle.dumps(obj, pickle_protocol)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
            "    cp.dump(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n",
            "    return Pickler.dump(self, obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 462, in __getnewargs__\n",
            "    raise RuntimeError(\n",
            "RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PicklingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     72\u001b[0m             )\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;31m# This method is called when attempting to pickle SparkContext, which is always an error:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    463\u001b[0m             \u001b[0;34m\"It appears that you are attempting to reference SparkContext from a broadcast \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-7dfea98d93cd>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcases_rdd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrowwise_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Convert RDD Back to DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcasesNewDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcases_rdd_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mcasesNewDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1274\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             )\n\u001b[0;32m-> 1276\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m   1277\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    929\u001b[0m         \"\"\"\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0mtupled_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \"\"\"\n\u001b[0;32m--> 874\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSized\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The first row in RDD is empty, can not infer schema\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2867\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2868\u001b[0m         \"\"\"\n\u001b[0;32m-> 2869\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2870\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2871\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2835\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2836\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2838\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   2317\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2318\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2319\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2320\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5439\u001b[0m             \u001b[0mprofiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5441\u001b[0;31m         wrapped_func = _wrap_function(\n\u001b[0m\u001b[1;32m   5442\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_jrdd_deserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5443\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   5239\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"serializer should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5240\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5241\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5242\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5243\u001b[0m     return sc._jvm.SimplePythonFunction(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   5222\u001b[0m     \u001b[0;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5223\u001b[0m     \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5224\u001b[0;31m     \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5225\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBroadcastThreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Default 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bytes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgUNm-ar5x2y",
        "outputId": "f49f30dc-5e52-46c9-a1a0-1c36371aaf08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+--------------+--------------------+--------------+-------------+--------+--------------------+--------------------+--------------------+\n",
            "|platform|      utc_event_time|utc_event_date|             user_id|    event_type|ecom.price100|ecom.qty|             ecom.nm|       main_category|        sub_category|\n",
            "+--------+--------------------+--------------+--------------------+--------------+-------------+--------+--------------------+--------------------+--------------------+\n",
            "|    Site|2023-07-31 20:52:...|    2023-07-31|61896930866132383...|ec.add_to_cart|      [27700]|     [1]|[2081100339580357...|[6968191755455670...|[1664831343325037...|\n",
            "|    Site|2023-07-31 20:54:...|    2023-07-31|61896930866132383...|ec.add_to_cart|      [20100]|     [1]|[2150579222891727...|[6968191755455670...|[1664831343325037...|\n",
            "|    Site|2023-07-31 20:58:...|    2023-07-31|61896930866132383...|ec.add_to_cart|      [27700]|     [1]|[2081100339580357...|[6968191755455670...|[1664831343325037...|\n",
            "|    Site|2023-07-31 21:00:...|    2023-07-31|61896930866132383...|ec.add_to_cart|      [22000]|     [1]|[2150579222891727...|[6968191755455670...|[1664831343325037...|\n",
            "|    Site|2023-07-31 21:02:...|    2023-07-31|61896930866132383...|ec.add_to_cart|      [28400]|     [1]|[1381949702717470...|[2217261702143726...|[1646384211525656...|\n",
            "|    Site|2023-07-31 21:02:...|    2023-07-31|61896930866132383...|ec.add_to_cart|      [28200]|     [1]|[1864591665048506...|[2217261702143726...|[1646384211525656...|\n",
            "|    Site|2023-07-31 21:03:...|    2023-07-31|61896930866132383...|ec.add_to_cart|      [28100]|     [1]|[2643699655495342...|[2217261702143726...|[1646384211525656...|\n",
            "|    Site|2023-07-31 19:45:...|    2023-07-31|44175339655536649...|ec.add_to_cart|      [16000]|     [1]|[1286176126709755...|[7140277177154559...|[1322138964595736...|\n",
            "|    Site|2023-07-31 19:48:...|    2023-07-31|44175339655536649...|ec.add_to_cart|      [19500]|     [1]|[1536412690632417...|[7140277177154559...|[1322138964595736...|\n",
            "|    Site|2023-07-31 20:22:...|    2023-07-31|91527446414899249...|ec.add_to_cart|      [78600]|     [1]|[6794762828812030...|[7928322154543508...|[3981136185780624...|\n",
            "+--------+--------------------+--------------+--------------------+--------------+-------------+--------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "path_old = '/content/ab_data_old.csv'\n",
        "path_new = '/content/ab_data_new.csv'\n",
        "\n",
        "df_old = spark.read.csv(path_old, header=True)\n",
        "df_new = spark.read.csv(path_new, header=True)\n",
        "df = df_old.unionByName(df_new, allowMissingColumns=True)\n",
        "# df = df.repartitionByRange(200, \"utc_event_date\")\n",
        "# print('Rows: ', df.count())\n",
        "df.limit(10).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tlzKwHaJXKh"
      },
      "source": [
        "Changing column types & names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEBYnhmODYmO",
        "outputId": "9e1f7635-af0a-4269-d862-14addf041866"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31m\"DataFrame before\"\u001b[0m\n",
            "root\n",
            " |-- platform: string (nullable = true)\n",
            " |-- utc_event_time: string (nullable = true)\n",
            " |-- utc_event_date: string (nullable = true)\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- event_type: string (nullable = true)\n",
            " |-- ecom.price100: string (nullable = true)\n",
            " |-- ecom.qty: string (nullable = true)\n",
            " |-- ecom.nm: string (nullable = true)\n",
            " |-- main_category: string (nullable = true)\n",
            " |-- sub_category: string (nullable = true)\n",
            "\n",
            "\u001b[31m\"DataFrame after\"\u001b[0m\n",
            "root\n",
            " |-- platform: string (nullable = true)\n",
            " |-- utc_event_time: string (nullable = true)\n",
            " |-- utc_event_date: string (nullable = true)\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- event_type: string (nullable = true)\n",
            " |-- price: string (nullable = true)\n",
            " |-- quantity: string (nullable = true)\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- main_category: string (nullable = true)\n",
            " |-- sub_category: string (nullable = true)\n",
            "\n",
            "--- 0.29206395149230957 seconds ---\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "print(\"\\x1b[31m\\\"DataFrame before\\\"\\x1b[0m\")\n",
        "df.printSchema()\n",
        "\n",
        "#changing names\n",
        "new_names = ['platform', 'utc_event_time','utc_event_date','user_id','event_type','price','quantity','product_id','main_category','sub_category']\n",
        "df = df.toDF(*new_names)\n",
        "\n",
        "#deleting '[' and ']' and '.' from str data\n",
        "for col in ['user_id',\"price\",\"quantity\",\"product_id\",'main_category','sub_category']:\n",
        "  df = delete_brackets(df, col)\n",
        "df = df.withColumn('event_type', F.regexp_replace('event_type', 'ec.', ''))\n",
        "\n",
        "print(\"\\x1b[31m\\\"DataFrame after\\\"\\x1b[0m\")\n",
        "df.printSchema()\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zp24oZ2l2-j"
      },
      "source": [
        "Counting Null/NaN values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ_6YpKEbzHp"
      },
      "outputs": [],
      "source": [
        "# start_time = time.time()\n",
        "# df1 = df.select('utc_event_time','user_id','price','quantity', 'product_id', 'main_category','sub_category')\n",
        "# df2 = df.select([c for c in df.columns if c not in ['utc_event_time','user_id','price','quantity', 'product_id']])\n",
        "\n",
        "# df1.select([(F.count(F.when(F.isnan(c) | F.col(c).isNull(), c))/F.count(F.lit(1))).alias(c) for c in df1.columns]).show()\n",
        "# df2.select([(F.count(F.when(F.col(c).isNull(), c))/F.count(F.lit(1))).alias(c) for c in df2.columns]).show()\n",
        "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIejmxqZVJXq"
      },
      "outputs": [],
      "source": [
        "#Drop missing values\n",
        "df = df.dropna(how='any')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nrookBfQeJn"
      },
      "source": [
        "Checking 'quantity' for reasonable values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DuO2T44Pz3r"
      },
      "outputs": [],
      "source": [
        "#df.select('quantity').distinct().collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPsZ11FwQjbu"
      },
      "outputs": [],
      "source": [
        "# replace 0 with 1\n",
        "df = df.withColumn(\"quantity\", F.when(F.col(\"quantity\") == 0, 1).otherwise(F.col(\"quantity\")))\n",
        "# df.select('quantity').distinct().collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psHRDsE6QjuQ"
      },
      "source": [
        "Checking 'price' for reasonable values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA5eajTAQkvC"
      },
      "outputs": [],
      "source": [
        "# df.select('price').describe().show()\n",
        "# q25 = df.approxQuantile([\"price\"], [0.25], 0.1)[0][0]\n",
        "# q75 = df.approxQuantile([\"price\"], [0.75], 0.1)[0][0]\n",
        "# q90 = df.approxQuantile([\"price\"], [0.90], 0.1)[0][0]\n",
        "# q95 = df.approxQuantile([\"price\"], [0.95], 0.1)[0][0]\n",
        "# print('25% значение - ', q25)\n",
        "# print('75% значение - ', q75)\n",
        "# print('90% значение - ', q90)\n",
        "# print('95% значение - ', q95)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq74UdnKR9_U"
      },
      "outputs": [],
      "source": [
        "# limit upper boundary to 0.9 percentile\n",
        "# df = df.withColumn(\"price\", F.col(\"price\").cast(IntegerType()))\n",
        "# q90 = df.approxQuantile([\"price\"], [0.90], 0.2)[0][0]\n",
        "# df = df.withColumn(\"price\", F.when(F.col(\"price\") >= q90, q90).otherwise(F.col(\"price\")))\n",
        "# df = df.withColumn(\"price\", F.col(\"price\").cast(IntegerType()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f2xq5L2QswV",
        "outputId": "3845d461-7859-4aee-bdca-f85f1db27d7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+--------------+--------------------+-----------+------+--------+--------------------+--------------------+--------------------+\n",
            "|platform|      utc_event_time|utc_event_date|             user_id| event_type| price|quantity|          product_id|       main_category|        sub_category|\n",
            "+--------+--------------------+--------------+--------------------+-----------+------+--------+--------------------+--------------------+--------------------+\n",
            "|    Site|2023-07-31 20:52:...|    2023-07-31|61896930866132383...|add_to_cart| 27700|       1| 2081100339580357...| 6968191755455670...| 1664831343325037...|\n",
            "|    Site|2023-07-31 20:54:...|    2023-07-31|61896930866132383...|add_to_cart| 20100|       1| 2150579222891727...| 6968191755455670...| 1664831343325037...|\n",
            "|    Site|2023-07-31 20:58:...|    2023-07-31|61896930866132383...|add_to_cart| 27700|       1| 2081100339580357...| 6968191755455670...| 1664831343325037...|\n",
            "|    Site|2023-07-31 21:00:...|    2023-07-31|61896930866132383...|add_to_cart| 22000|       1| 2150579222891727...| 6968191755455670...| 1664831343325037...|\n",
            "|    Site|2023-07-31 21:02:...|    2023-07-31|61896930866132383...|add_to_cart| 28400|       1| 1381949702717470...| 2217261702143726...| 1646384211525656...|\n",
            "|    Site|2023-07-31 21:02:...|    2023-07-31|61896930866132383...|add_to_cart| 28200|       1| 1864591665048506...| 2217261702143726...| 1646384211525656...|\n",
            "|    Site|2023-07-31 21:03:...|    2023-07-31|61896930866132383...|add_to_cart| 28100|       1| 2643699655495342...| 2217261702143726...| 1646384211525656...|\n",
            "|    Site|2023-07-31 19:45:...|    2023-07-31|44175339655536649...|add_to_cart| 16000|       1| 1286176126709755...| 7140277177154559...| 1322138964595736...|\n",
            "|    Site|2023-07-31 19:48:...|    2023-07-31|44175339655536649...|add_to_cart| 19500|       1| 1536412690632417...| 7140277177154559...| 1322138964595736...|\n",
            "|    Site|2023-07-31 20:22:...|    2023-07-31|91527446414899249...|add_to_cart| 78600|       1| 6794762828812030...| 7928322154543508...| 3981136185780624...|\n",
            "+--------+--------------------+--------------+--------------------+-----------+------+--------+--------------------+--------------------+--------------------+\n",
            "\n",
            "--- 1.0571339130401611 seconds for printing ---\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "df.limit(10).show()\n",
        "print(\"--- %s seconds for printing ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iC7zhsn9eLW"
      },
      "source": [
        "## Adding new features <a class=\"anchor\" id=\"feature\"></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAdxMqFds-W4"
      },
      "source": [
        "- <b>session duration </b>\n",
        "\n",
        "  We consider a session - continuous interactions with the markeplace within 30 min. If event_k - event_n > 30, event_k is a start of a new session.\n",
        "\n",
        "  For each session we count its duration.\n",
        "- <b>is_first_event_in_session</b>\n",
        "\n",
        "  Marking each event as a starting (true) or not (false)\n",
        "- <b>session_id</b>\n",
        "\n",
        "  Giving each session unique index\n",
        "\n",
        "\n",
        "- <b>avg_session_duration</b>\n",
        "\n",
        "  Average session duration per user (sum of unique durations/ sum unique sessions)\n",
        "\n",
        "- <b>day_time </b>\n",
        "\n",
        "  Specifying utc_event_time on\n",
        "\n",
        "  - Early Morning: (06.00.00 - 09.00.00)\n",
        "\n",
        "  - Morning: (09.00.00 - 12.00.00)\n",
        "\n",
        "  - Afternoon: (12.00.00 - 17.00.00)\n",
        "\n",
        "  - Evening: (17.00.00 - 24.00.00)\n",
        "\n",
        "  - Night (00.00.00 - 06.00.00)\n",
        "\n",
        "- <b>day_of_week</b>\n",
        "\n",
        "  Derive a day from the date and write in number\n",
        "\n",
        "- <b>mean_product_price</b>\n",
        "\n",
        "  Average product price the user has interected with\n",
        "\n",
        "- <b>inter_categ</b>\n",
        "\n",
        "  Number of categories the user paid attention to\n",
        "\n",
        "- <b>inter_goods</b>\n",
        "\n",
        "  Number of products the user paid attention to\n",
        "\n",
        "- <b>avg_goods_per_session </b>\n",
        "\n",
        "  Average Number of products the user paid attention to throught his sessions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = split_on_sessions(df)\n",
        "df = avg_session_duration(df)\n",
        "df = day_time(df)\n",
        "df = day_of_week(df)\n",
        "df = mean_product_price_per_person(df)\n",
        "df = interacted_categories_per_person(df)\n",
        "df = interacted_goods_per_person(df)\n",
        "df = avg_goods_per_person(df)\n",
        "# df= product_popularity(df)\n",
        "\n",
        "\n",
        "int_cols = ['price','quantity','day_of_week']\n",
        "date_col = ['utc_event_time','utc_event_date','session_duration','avg_session_duration']\n",
        "string_cols = ['platform','user_id','product_id','main_category','sub_category','event_type']\n",
        "\n",
        "df = change_dtype(df,int_cols=int_cols, string_cols=string_cols, date_cols=date_col)\n",
        "\n",
        "df.write.mode(\"overwrite\").saveAsTable('df_tab', format=\"parquet\")\n",
        "df = spark.read.table('df_tab')"
      ],
      "metadata": {
        "id": "ye6XxszBr4Vw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a9acb191-3444-44dd-9e0e-a4e728afeffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-8b95ce44968d>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchange_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstring_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'df_tab'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'df_tab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1519\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1521\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m     def json(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o477.saveAsTable.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 158 in stage 24.0 failed 1 times, most recent failure: Lost task 158.0 in stage 24.0 (TID 1822) (a43798bc1b7a executor driver): java.io.IOException: No space left on device\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:543)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.io.IOException: No space left on device\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:354)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat org.apache.spark.io.MutableCheckedOutputStream.write(MutableCheckedOutputStream.scala:43)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:225)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:178)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:543)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:310)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:171)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNO3m1tyPyWv"
      },
      "source": [
        "## ML pipeline <a class=\"anchor\" id=\"pipeline\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JdFJivTZVrgd"
      },
      "outputs": [],
      "source": [
        "cat_columns = ['platform','event_type','day_time','day_of_week']\n",
        "num_cols = ['price','quantity','session_duration','avg_session_duration','mean_price','inter_categ','inter_goods','avg_goods_per_session']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0Xoy0YBhWJ8T"
      },
      "outputs": [],
      "source": [
        "stages = []\n",
        "\n",
        "#OHE of cat_cols\n",
        "for cat_column in cat_columns:\n",
        "  stringIndexer = StringIndexer(inputCol = cat_column, outputCol = cat_column +'_idx')\n",
        "  encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()],outputCols=[cat_column +'_categ'])\n",
        "\n",
        "  stages += [stringIndexer, encoder]\n",
        "\n",
        "# indexing\n",
        "stages += [StringIndexer(inputCol = 'is_first_event_in_session', outputCol = 'is_first_event_in_session' +'_idx')]\n",
        "#qcut of price in 5 buckets\n",
        "price_buckets = QuantileDiscretizer(numBuckets=5, inputCol='price', outputCol='price' + \"bucket\")\n",
        "stages += [price_buckets]\n",
        "\n",
        "#scaling num_cols\n",
        "for num_col in num_cols:\n",
        "  scaler = StandardScaler(inputCol=num_col, outputCol=num_col + \"_scal\",\n",
        "                          withStd=True, withMean=False)\n",
        "  stages += [scaler]\n",
        "\n",
        "#assembling\n",
        "\n",
        "assemble_inputs = [c + '_categ' for c in cat_columns] + num_cols\n",
        "assembler = VectorAssembler(inputCols = assemble_inputs, outputCol = 'features')\n",
        "stages += [assembler]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Bw5ElDeI-5-z"
      },
      "outputs": [],
      "source": [
        "pipeline = Pipeline(stages=stages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FpU0xv8xbiv"
      },
      "source": [
        "## Statistics on days <a class=\"anchor\" id=\"stat\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiBELrkwxbiw"
      },
      "outputs": [],
      "source": [
        "expr = [F.countDistinct(F.col('user_id')), F.count(F.col('event_type'))]\n",
        "df_time = df.groupBy(F.col(\"utc_event_date\")).agg(*expr)\n",
        "pdf_time = df_time.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVWcKYL8xbix"
      },
      "outputs": [],
      "source": [
        "df1 = df.groupBy('utc_event_date').agg(F.collect_set(F.col('user_id')).alias('unique_users_id'))\n",
        "\n",
        "my_window = Window.orderBy(\"utc_event_date\")\n",
        "df1 = df1.withColumn(\"prev_users\", F.lag((F.col(\"unique_users_id\"))).over(my_window))\n",
        "df1 = df1.withColumn(\"future_users\", F.lead((F.col(\"unique_users_id\"))).over(my_window))\n",
        "df1 = df1.withColumn(\"prev_inters\", F.array_intersect(F.col('prev_users'), F.col('unique_users_id')))\n",
        "df1 = df1.withColumn(\"future_inters\", F.array_intersect(F.col('future_users'), F.col('unique_users_id')))\n",
        "\n",
        "for row in ['unique_users_id','prev_inters','future_inters']:\n",
        "  df1 = df1.withColumn(row, F.size(F.col(row)))\n",
        "\n",
        "df1 = df1.withColumn('previous_overlap', F.round(F.col('prev_inters') / F.col('unique_users_id'),2))\n",
        "df1 = df1.withColumn('future_overlap', F.round(F.col('future_inters') / F.col('unique_users_id'),2))\n",
        "pdf1 = df1.select(\"utc_event_date\",'previous_overlap','future_overlap').toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJs_AFh4xbiz",
        "outputId": "73aa24ad-9509-443e-d0f9-8d30b215fd61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min date: 2023-07-31\n",
            "Max date: 2023-08-28\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a58bed2a-8d7b-4524-ae5e-a74096c780f8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>utc_event_date</th>\n",
              "      <th>2023-07-31</th>\n",
              "      <th>2023-08-01</th>\n",
              "      <th>2023-08-02</th>\n",
              "      <th>2023-08-03</th>\n",
              "      <th>2023-08-04</th>\n",
              "      <th>2023-08-05</th>\n",
              "      <th>2023-08-06</th>\n",
              "      <th>2023-08-07</th>\n",
              "      <th>2023-08-08</th>\n",
              "      <th>2023-08-09</th>\n",
              "      <th>...</th>\n",
              "      <th>2023-08-19</th>\n",
              "      <th>2023-08-20</th>\n",
              "      <th>2023-08-21</th>\n",
              "      <th>2023-08-22</th>\n",
              "      <th>2023-08-23</th>\n",
              "      <th>2023-08-24</th>\n",
              "      <th>2023-08-25</th>\n",
              "      <th>2023-08-26</th>\n",
              "      <th>2023-08-27</th>\n",
              "      <th>2023-08-28</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>previous_overlap</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.57</td>\n",
              "      <td>...</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>future_overlap</th>\n",
              "      <td>0.55</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.57</td>\n",
              "      <td>...</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>count(user_id)</th>\n",
              "      <td>116709.00</td>\n",
              "      <td>114509.00</td>\n",
              "      <td>113913.00</td>\n",
              "      <td>113740.00</td>\n",
              "      <td>108470.00</td>\n",
              "      <td>101025.00</td>\n",
              "      <td>106932.00</td>\n",
              "      <td>113279.00</td>\n",
              "      <td>115504.00</td>\n",
              "      <td>115474.00</td>\n",
              "      <td>...</td>\n",
              "      <td>109438.00</td>\n",
              "      <td>115354.00</td>\n",
              "      <td>121534.00</td>\n",
              "      <td>123532.00</td>\n",
              "      <td>122131.00</td>\n",
              "      <td>118955.00</td>\n",
              "      <td>117778.00</td>\n",
              "      <td>113898.00</td>\n",
              "      <td>118328.00</td>\n",
              "      <td>123822.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>count(event_type)</th>\n",
              "      <td>3451387.00</td>\n",
              "      <td>3393743.00</td>\n",
              "      <td>3379766.00</td>\n",
              "      <td>3405500.00</td>\n",
              "      <td>3143646.00</td>\n",
              "      <td>3031366.00</td>\n",
              "      <td>3383943.00</td>\n",
              "      <td>3389026.00</td>\n",
              "      <td>3576467.00</td>\n",
              "      <td>3539820.00</td>\n",
              "      <td>...</td>\n",
              "      <td>3416461.00</td>\n",
              "      <td>3751232.00</td>\n",
              "      <td>3594258.00</td>\n",
              "      <td>3772552.00</td>\n",
              "      <td>3715644.00</td>\n",
              "      <td>3545791.00</td>\n",
              "      <td>3407302.00</td>\n",
              "      <td>3430817.00</td>\n",
              "      <td>3609939.00</td>\n",
              "      <td>3562750.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4 rows × 29 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a58bed2a-8d7b-4524-ae5e-a74096c780f8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a58bed2a-8d7b-4524-ae5e-a74096c780f8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a58bed2a-8d7b-4524-ae5e-a74096c780f8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8d7c58e2-5a92-4b95-9c78-b426dbba61bd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8d7c58e2-5a92-4b95-9c78-b426dbba61bd')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8d7c58e2-5a92-4b95-9c78-b426dbba61bd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "utc_event_date     2023-07-31  2023-08-01  2023-08-02  2023-08-03  2023-08-04  \\\n",
              "previous_overlap         0.00        0.56        0.56        0.56        0.57   \n",
              "future_overlap           0.55        0.55        0.55        0.55        0.52   \n",
              "count(user_id)      116709.00   114509.00   113913.00   113740.00   108470.00   \n",
              "count(event_type)  3451387.00  3393743.00  3379766.00  3405500.00  3143646.00   \n",
              "\n",
              "utc_event_date     2023-08-05  2023-08-06  2023-08-07  2023-08-08  2023-08-09  \\\n",
              "previous_overlap         0.56        0.53        0.53        0.56        0.57   \n",
              "future_overlap           0.57        0.57        0.57        0.57        0.57   \n",
              "count(user_id)      101025.00   106932.00   113279.00   115504.00   115474.00   \n",
              "count(event_type)  3031366.00  3383943.00  3389026.00  3576467.00  3539820.00   \n",
              "\n",
              "utc_event_date     ...  2023-08-19  2023-08-20  2023-08-21  2023-08-22  \\\n",
              "previous_overlap   ...        0.56        0.54        0.54        0.56   \n",
              "future_overlap     ...        0.57        0.57        0.57        0.57   \n",
              "count(user_id)     ...   109438.00   115354.00   121534.00   123532.00   \n",
              "count(event_type)  ...  3416461.00  3751232.00  3594258.00  3772552.00   \n",
              "\n",
              "utc_event_date     2023-08-23  2023-08-24  2023-08-25  2023-08-26  2023-08-27  \\\n",
              "previous_overlap         0.57        0.57        0.57        0.56        0.55   \n",
              "future_overlap           0.56        0.56        0.54        0.57        0.57   \n",
              "count(user_id)      122131.00   118955.00   117778.00   113898.00   118328.00   \n",
              "count(event_type)  3715644.00  3545791.00  3407302.00  3430817.00  3609939.00   \n",
              "\n",
              "utc_event_date     2023-08-28  \n",
              "previous_overlap         0.54  \n",
              "future_overlap           0.00  \n",
              "count(user_id)      123822.00  \n",
              "count(event_type)  3562750.00  \n",
              "\n",
              "[4 rows x 29 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f\"Min date: {pdf_time['utc_event_date'].min()}\\nMax date: {pdf_time['utc_event_date'].max()}\")\n",
        "pdf1.merge(pdf_time, how='left', on ='utc_event_date').set_index('utc_event_date').T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deOQ0AXpxbi0"
      },
      "source": [
        "__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlxl6dQSxbi1"
      },
      "source": [
        "* Всего имеем данные за 3 недели (21 дней).\n",
        "* Распределение пользователей и их активноcтей одинаково в каждом дне.\n",
        "* Пересечение пользователей по дням одинаково.\n",
        "* 80% (17 дней) отправляем на обучение , на остальных 20% (4 днях) будет тестировать модели.\n",
        "* 17 дней разделим ещё на 2 части: по 13 и 4 дней для обучения и валидации модели 1 и 2 уровня"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2spA8PWxbi1"
      },
      "source": [
        "__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdzaagKEPByh"
      },
      "source": [
        "## Splitting <a class=\"anchor\" id=\"split\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "p1ZfnXwuowF1"
      },
      "outputs": [],
      "source": [
        "#global train-test\n",
        "split_date = df_new.select(F.to_timestamp(F.max(F.col(\"utc_event_date\")))).collect()[0][0] - datetime.timedelta(days=4)\n",
        "global_train = df_new.where(df_new.utc_event_date <= split_date)\n",
        "global_test = df_new.where(df_new.utc_event_date > split_date)\n",
        "global_train.persist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2sucC2dhb0D"
      },
      "source": [
        "## Creating Model <a class=\"anchor\" id=\"model\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xaXxBJFm9AjE"
      },
      "outputs": [],
      "source": [
        "global_train.show(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UprqI5Fh9GSv"
      },
      "outputs": [],
      "source": [
        "pipeline_model = pipeline.fit(global_train)\n",
        "train_prep = pipeline_model.transform(global_train)\n",
        "test_prep = pipeline_model.transform(global_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "xVFTBv5uIF7Q",
        "iTiN8vAWqNpG",
        "kNO3m1tyPyWv",
        "6FpU0xv8xbiv"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}